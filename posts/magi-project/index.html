<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>MAGI Project - Raptor Blog</title><meta name=Description content="Write up about the process of creating my personal Kubernetes cluster"><meta property="og:title" content="MAGI Project"><meta property="og:description" content="Write up about the process of creating my personal Kubernetes cluster"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.anthares101.com/posts/magi-project/"><meta property="og:image" content="https://blog.anthares101.com/Assembly1.jpg"><meta property="og:image" content="https://blog.anthares101.com/Assembly2.jpg"><meta property="og:image" content="https://blog.anthares101.com/Assembly3.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-09T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-09T00:00:00+00:00"><meta property="og:site_name" content="Raptor Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.anthares101.com/Assembly1.jpg"><meta name=twitter:title content="MAGI Project"><meta name=twitter:description content="Write up about the process of creating my personal Kubernetes cluster"><meta name=application-name content="Raptor Blog"><meta name=apple-mobile-web-app-title content="Raptor Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://blog.anthares101.com/posts/magi-project/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"MAGI Project","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/blog.anthares101.com\/posts\/magi-project\/"},"genre":"posts","keywords":"kubernetes, blue team, homelab","wordcount":4606,"url":"https:\/\/blog.anthares101.com\/posts\/magi-project\/","datePublished":"2022-02-09T00:00:00+00:00","dateModified":"2022-08-09T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Anthares101"},"description":"Write up about the process of creating my personal Kubernetes cluster"}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Raptor Blog"><span class=header-title-pre><i class='fa-solid fa-flag'></i></span>Raptor Blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=https://github.com/anthares101/raptor-blog title="Check the code!" rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><a class=menu-item href=https://anthares101.com title="Anthares WriteUps" rel="noopener noreffer" target=_blank><i class='fa-solid fa-file-lines'></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Raptor Blog"><span class=header-title-pre><i class='fa-solid fa-flag'></i></span>Raptor Blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=https://github.com/anthares101/raptor-blog title="Check the code!" rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a class=menu-item href=https://anthares101.com title="Anthares WriteUps" rel="noopener noreffer" target=_blank><i class='fa-solid fa-file-lines'></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">MAGI Project</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/posts/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Anthares101</a></span>&nbsp;<span class=post-category>included in <a href=/categories/infrastructure/><i class="far fa-folder fa-fw" aria-hidden=true></i>Infrastructure</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-02-09>2022-02-09</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;4606 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;22 minutes&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/Assembly3.jpg data-srcset="/posts/magi-project/images/Assembly3.jpg, /posts/magi-project/images/Assembly3.jpg 1.5x, /posts/magi-project/images/Assembly3.jpg 2x" data-sizes=auto alt=/posts/magi-project/images/Assembly3.jpg title="Write up about the process of creating my personal Kubernetes cluster" width=1280 height=960></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#the-idea>The idea</a><ul><li><a href=#what-are-you-going-to-read>What are you going to read</a></li></ul></li><li><a href=#assembly>Assembly</a></li><li><a href=#preparing-the-sd-cards>Preparing the SD cards</a></li><li><a href=#initial-setup>Initial setup</a></li><li><a href=#cluster-setup>Cluster setup</a><ul><li><a href=#basic-raspberrys-setup>Basic Raspberrys setup</a></li><li><a href=#installing-kubernetes>Installing Kubernetes</a><ul><li><a href=#master-node>Master node</a></li><li><a href=#worker-nodes>Worker nodes</a></li></ul></li><li><a href=#storage-provisioning>Storage provisioning</a><ul><li><a href=#nfs-server-installation>NFS server installation</a></li></ul></li><li><a href=#cluster-configuration>Cluster configuration</a><ul><li><a href=#taints-and-labels>Taints and labels</a></li><li><a href=#nfs-storage-provider>NFS storage provider</a></li><li><a href=#traefik-custom-install>Traefik custom install</a></li><li><a href=#monitoring>Monitoring</a></li><li><a href=#network-policies>Network Policies</a></li><li><a href=#handling-certificates-and-application-security>Handling certificates and application security</a></li></ul></li></ul></li><li><a href=#some-tips>Some tips</a><ul><li><a href=#importing-images-to-k3s>Importing images to K3S</a></li><li><a href=#k3s-snapshots>K3S snapshots</a></li></ul></li><li><a href=#last-thoughts>Last Thoughts</a></li></ul></nav></div></div><div class=content id=content><h1 id=magi-project>MAGI project</h1><h2 id=the-idea>The idea</h2><p>I have been thinking about the idea of building a cluster made of Raspberrys and update my home infrastructure a bit. Right now, I use a Raspberry Pi 4 B as a little server to run Pi Hole, personal projects, Plex&mldr; and also as a NAS. All this sevices are run using Docker containers because I love Docker and the management is easier that way, allowing me to recover from a failure really quick.</p><p>Since I’m increasing the load to it with more things and some application like Plex sometimes consumes A LOT a cluster could be awesome. After thinking a bit my options I discovered PicoCluster:</p><figure><a href=https://www.picocluster.com/ class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>PicoCluster - Desktop Micro Data Center</div><div class=bookmark-description>Kubernetes Docker Cluster Software Advanced Kits and Assembled Cubes come with 4GB Raspberry 4 boards. Starter Kits support 1GB, 2GB and 4GB boards.</div></div></div><img src="https://cdn.shopify.com/s/files/1/1214/6676/files/PicoClusterLogoFinal_150.png?height=628&pad_color=fff&v=1494390559&width=1200" class=bookmark-image></a></figure><p>They have a lot of cool things and one of them is exactly what I needed, a little cluster with 3 Raspberrys. I bought it, if it is not obvious by now, and the plan is to migrate some of the things I have running in my actual Pi to it.</p><p>The idea is simple, my initial Raspberry will only have essential network services like Pi Hole (Working as DHCP and DNS server), Samba (Used for backups and to add things to Plex) and Netdata for monitoring it 24/7 and alert me if something is going wrong. Meanwhile, the cluster will use that Pi for storage provisioning and will host the rest of application like Plex, Nextcloud, personal projects&mldr;</p><p>Also, as part of this project I want to make sure I can still recover from a terminal failure without a ton of problems and learn a bit more of Ansible. The idea here is that all the cluster initial installation and setup will be performed by an Ansible Playbook that I will create from scratch.</p><h3 id=what-are-you-going-to-read>What are you going to read</h3><p>Anthares from the future here! The initial idea sounds great right? A cluster of Raspberrys what a cool thing! Well, looks like migrating all to Kubernetes from a Docker setup is not as straightforward as I thought. Also, while I was working on it I started thinking about improving the security of the new infrastructure and fix some problems I had taking advantage of some Kubernetes features.</p><p>What I though It was going to be a fast and smooth process (I already have worked with Kubernetes and Helm before) ended up being a really long journey of reading documentation and unexpected problems BUT I really learnt a lot.</p><p>I tried to cover all the process I followed during my journey so I invite you to relax, get a coffee or something and join me in this adventure, Its going to be a long one. Keep in mind that this is not a Kubernetes tutorial, if you want to follow my steps you are supposed to already know a bit about how things works in Kubernetes.</p><p>Oh! I almost forgot about it. Keep in mind that the 64 bit version of Raspbian was just released yesterday, 02/04/2022, so obviously during the installation process of this write up I had to install a pre-release version of it.</p><h2 id=assembly>Assembly</h2><p>After waiting a bit (I’m in Spain so it is a long way from the US) a received my order!</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/Assembly1.jpg data-srcset="/posts/magi-project/images/Assembly1.jpg, /posts/magi-project/images/Assembly1.jpg 1.5x, /posts/magi-project/images/Assembly1.jpg 2x" data-sizes=auto alt=/posts/magi-project/images/Assembly1.jpg title=Assembly1.jpg width=1280 height=960></p><p>You can ask them to do the assembly process or even install the applications you want into the cluster to avoid wasting time setting it up but&mldr; that is not fun right?</p><p>The assembly process is documented fairly well in their site and you will only need a screwdriver and some different size heads for it in order to follow all the process.</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/Assembly2.jpg data-srcset="/posts/magi-project/images/Assembly2.jpg, /posts/magi-project/images/Assembly2.jpg 1.5x, /posts/magi-project/images/Assembly2.jpg 2x" data-sizes=auto alt=/posts/magi-project/images/Assembly2.jpg title=Assembly2.jpg width=1280 height=960></p><p>By the way, you can ask them to only send the parts for the cluster but not the actual boards for it. The thing is that with the chip shortage we have right now the price they sell them for is just too good to not get all from them.</p><p>The assembly took me around 2 hours or so but it was pretty fun to do and it looks incredible:</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/Assembly3.jpg data-srcset="/posts/magi-project/images/Assembly3.jpg, /posts/magi-project/images/Assembly3.jpg 1.5x, /posts/magi-project/images/Assembly3.jpg 2x" data-sizes=auto alt=/posts/magi-project/images/Assembly3.jpg title=Assembly3.jpg width=1280 height=960></p><h2 id=preparing-the-sd-cards>Preparing the SD cards</h2><p>We have a cool looking cube right now, time to bring it to life! The first thing to do is to prepare the SD cards with Raspbian and a basic headless setup (No monitors please).</p><p>In this case I want to use the 64 bit version of Raspbian (Even though it is still under development and have some issues) because since Apple started using ARM a lot of applications work with ARM64 and can be handy. This Raspbian version can be downloaded from <a href=https://downloads.raspberrypi.org/raspios_arm64/images/ target=_blank rel="noopener noreffer">here</a>.</p><p>I will use the Raspberry Pi imager program to put Raspbian in the SD cards. You can find this software here:</p><figure><a href=https://www.raspberrypi.com/software/ class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>Raspberry Pi OS - Raspberry Pi</div><div class=bookmark-description>From industries large and small, to the kitchen table tinkerer, to the classroom coder, we make computing accessible and affordable for everybody.</div></div></div><img src=https://assets.raspberrypi.com/static/opengraph-6d80de9a444335c577028951fd1bab78.png class=bookmark-image></a></figure><p>There is an option in the program for custom images that works like a charm:</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/imager.png data-srcset="/posts/magi-project/images/imager.png, /posts/magi-project/images/imager.png 1.5x, /posts/magi-project/images/imager.png 2x" data-sizes=auto alt=/posts/magi-project/images/imager.png title=imager width=681 height=451></p><p>It will take a while just to prepare one card so imagine 3 of them.</p><p>Once the cards are prepared, we have to do one more thing. Since we don’t want to use a monitor we have to connect the cards to the computer, open the disk called <code>boot</code> and create a file called <code>ssh</code> . This will enable SSH by default.</p><p>The only problem right now is that the IP address for each Raspberry Pi will be provided by the DHCP server so in the first boot you will need to find them in the network.</p><h2 id=initial-setup>Initial setup</h2><p>My idea is to create an ansible playbook to make all the initial setup of the cluster, that way if a terminal failure happens I can just get everything setup in the blink of an eye.</p><p>But as I said above, the Raspberrys will have a random IP in the first boot so I will have to connect to them one by one to setup an static IP address and hostnames. After that, I can add SSH key authentication and from there I can start with the playbook to setup everything.</p><p>I know it is not perfect because if something goes wrong and I need to reinstall from a clean SD card, I will need to do this whole process of connecting to the Raspberry and make the very first setup by hand before I can use the playbook but I think it is ok for me.</p><p>Note: I had an error with the 64 bit Raspbian OS about the <code>locale</code> value. I just executed <code>sudo dpkg-reconfigure locales</code> and generated the language that the error was crying about.</p><h2 id=cluster-setup>Cluster setup</h2><p>All the steps described in this section will be included in an Ansible playbook to make this whole process automatic. You can check the playbook here:</p><figure><a href=https://github.com/anthares101/k3s-pi-cluster class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>GitHub - anthares101/k3s-pi-cluster: K3S Pi Cluster project playbook</div><div class=bookmark-description>The monitoring stack used is the Carlos Eduardo version of the kube-prometheus repo and part of the Ansible roles were adapted from Jeff Geerling turing-pi-cluster project.</div></div></div><img src=https://opengraph.githubassets.com/4075e0b5616e1dcbb8d015fab617a6f5699030f68f60e25994f092bc53ffa304/anthares101/k3s-pi-cluster class=bookmark-image></a></figure><h3 id=basic-raspberrys-setup>Basic Raspberrys setup</h3><p>Raspbian comes with some stuff configured that we don’t really need, Wi-Fi and Bluetooth for example. To disable them we can just add this to the <code>/boot/config.txt</code> file and reboot:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>dtoverlay=pi3-disable-wifi
</span></span><span class=line><span class=cl>dtoverlay=pi3-disable-bt
</span></span></code></pre></td></tr></table></div></div><p>The next thing is to prevent the default <code>pi</code> user from using the <code>sudo</code> command without a password. This is easy, just delete <code>/etc/sudoers.d/010_pi-nopasswd</code> .</p><p>Also, it would be awesome if the Raspberrys date is correct so set the correct timezone:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo timedatectl set-timezone &lt;your_time_zone&gt;
</span></span></code></pre></td></tr></table></div></div><p>For my last trick, I will make some changes to harden the system a bit. The home directory of the <code>pi</code> user is world readable so let’s change that:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>chmod <span class=m>0750</span> /home/pi
</span></span></code></pre></td></tr></table></div></div><p>And since we added a SSH key to the Raspberrys for SSH authentication I will disable the access through SSH using password and also won’t allow <code>root</code> user login. Adding this lines to <code>/etc/ssh/sshd_config</code> and reloading the <code>sshd</code> service will do:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>PermitRootLogin no
</span></span><span class=line><span class=cl>UsePAM no
</span></span><span class=line><span class=cl>PasswordAuthentication no
</span></span></code></pre></td></tr></table></div></div><h3 id=installing-kubernetes>Installing Kubernetes</h3><p>The Kubernetes version I will be installing is K3S:</p><figure><a href=https://k3s.io/ class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>Lightweight Kubernetes</div><div class=bookmark-description>The above figure shows the difference between K3s server and K3s agent nodes. For more information, see the architecture documentation. We are a Cloud Native Computing Foundation sandbox project.</div></div></div><img src=https://k3s.io/favicon.ico class=bookmark-image></a></figure><p>Should include all the normal Kubernetes features but in a more optimized way. The installation is pretty easy to do actually, it is documented very well in the K3S page and the only thing you have to do is to run an installation script in each node of the cluster.</p><p>The only thing to keep in mind is that you have to start with the installation of the master node (By default K3S won’t allow you to have more than one) to get a secret token you need for the worker nodes installation.</p><p>By default the K3S script will deploy some components to the cluster, for a basic testing environment this is ok but if you want to customize this components configuration you need to tell K3S to not deploy them to avoid getting your configuration getting overwrited in every reboot.</p><p>This page shows what parameters to use in order to customize the installation:</p><figure><a href=https://rancher.com/docs/k3s/latest/en/installation/install-options/server-config/ class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>K3s Server Configuration Reference</div><div class=bookmark-description>In this section, you'll learn how to configure the K3s server. Throughout the K3s documentation, you will see some options that can be passed in as both command flags and environment variables. For help with passing in options, refer to How to Use Flags and Environment Variables.</div></div></div><img src=https://rancher.com/docs/img/logo-square.png class=bookmark-image></a></figure><p>Before starting with the installation I will add this parameters into the <code>/boot/cmdline.txt</code> file of every Pi to make sure the containers work as expected:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cgroup_memory=1 cgroup_enable=memory
</span></span></code></pre></td></tr></table></div></div><h4 id=master-node>Master node</h4><p>Now we are ready to install K3S in the master node. Since I want to use Prometheus to get the metrics from the nodes instead of the typical Kubernetes metric-server I will use <code>-no-deploy metrics-server</code> to tell K3S to not deploy this component:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>./k3s_install.sh --no-deploy metrics-server
</span></span></code></pre></td></tr></table></div></div><p>Once this finish, make sure you get the token for the workers nodes: <code>/var/lib/rancher/k3s/server/node-token</code> and also the <code>kubeconfig</code> file to be able to manage your cluster: <code>/etc/rancher/k3s/k3s.yaml</code>.</p><h4 id=worker-nodes>Worker nodes</h4><p>To add you workers node to the cluster just execute this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>K3S_URL</span><span class=o>=</span>https://&lt;MASTER-NODE-IP&gt;:6443
</span></span><span class=line><span class=cl><span class=nv>K3S_TOKEN</span><span class=o>=</span>&lt;TOKEN&gt;
</span></span><span class=line><span class=cl>./k3s_install.sh
</span></span></code></pre></td></tr></table></div></div><h3 id=storage-provisioning>Storage provisioning</h3><p>For persistent volume provisioning I don’t really want to rely in the local path provider that K3S use by default. The problem with that provider is that if a pod is re-scheduled in a different node it won’t be able to access the persistent volume data because that volume is in another node. In order to avoid this problems, I will prepare a NFS server in the original Raspberry Pi I talked you about before and configure a NFS provider in the cluster that points to that server. The NFS server could be in one of the cluster nodes aswell if you don’t have another host to use.</p><h4 id=nfs-server-installation>NFS server installation</h4><p>To be honest the configuration of the a NFS server was easier than I expected. Just install <code>nfs-kernel-server</code> package and prepare a share folder to use.</p><p>In the <code>/etc/exports</code> is where you have to configure the share for NFS and also the IPs addreses that can access it. I will let here a little example:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>/home/pi/Shared MASTER_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) WORKER1_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) WORKER2_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000)
</span></span></code></pre></td></tr></table></div></div><p>Make sure to run <code>sudo exportfs -ra</code> to update NFS exports and also to check that <code>nfs-server</code> and <code>rpc-statd.service</code> are running.</p><h3 id=cluster-configuration>Cluster configuration</h3><h4 id=taints-and-labels>Taints and labels</h4><p>So we have a Kubernetes cluster but it needs a bit of tweaking</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/checkNodes.png data-srcset="/posts/magi-project/images/checkNodes.png, /posts/magi-project/images/checkNodes.png 1.5x, /posts/magi-project/images/checkNodes.png 2x" data-sizes=auto alt=/posts/magi-project/images/checkNodes.png title=checkNodes width=392 height=103></p><p>First of all, since I have 3 nodes I want to taint and label the master node to control what applications can be scheduled to it:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl taint nodes melchior CriticalAddonsOnly:NoSchedule
</span></span><span class=line><span class=cl>kubectl label node melchior node-type<span class=o>=</span>master
</span></span></code></pre></td></tr></table></div></div><p>I used that taint because I noticed that all the addons (That is the name K3S use for all the manifests that are put in the <code>/var/lib/rancher/k3s/server/manifests</code> folder for deployment) deployed by K3S use that toleration. The label is one I invented.</p><p>Tainting the master node will give me control over what pods are scheduled into the master node. The idea is to only allow critical application to be in the master node, this way the master node will be more protected against resource intensive applications that can cause a node to crash.</p><h4 id=nfs-storage-provider>NFS storage provider</h4><p>I will use this NFS provider:</p><figure><a href=https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>GitHub - kubernetes-sigs/nfs-subdir-external-provisioner</div><div class=bookmark-description>NFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}. Note: This repository is migrated from https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client.</div></div></div><img src=https://opengraph.githubassets.com/4a2dbaf767d197f27c33cb75833f8b2e416212aa24818be2855f2ac818b2dae0/kubernetes-sigs/nfs-subdir-external-provisioner class=bookmark-image></a></figure><p>Since it offers the installation through Helm and K3S accepts Helm as a way of deploying an addon, just copying this file to <code>/var/lib/rancher/k3s/server/manifests</code> will be enough:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>helm.cattle.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>HelmChart</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nfs-storage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>chart</span><span class=p>:</span><span class=w> </span><span class=l>nfs-subdir-external-provisioner</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>repo</span><span class=p>:</span><span class=w> </span><span class=l>https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>targetNamespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>set</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>nfs.server</span><span class=p>:</span><span class=w> </span><span class=l>NFS-SERVER-IP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>nfs.path</span><span class=p>:</span><span class=w> </span><span class=l>NFS-SHARE-PATH</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>storageClass.name</span><span class=p>:</span><span class=w> </span><span class=l>nfs-storage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>storageClass.accessModes</span><span class=p>:</span><span class=w> </span><span class=l>ReadWriteMany</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>storageClass.reclaimPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Retain</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>storageClass.archiveOnDelete</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;false&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>storageClass.defaultClass</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;true&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>valuesContent</span><span class=p>:</span><span class=w> </span><span class=p>|-</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    nodeSelector:
</span></span></span><span class=line><span class=cl><span class=sd>      node-type: master
</span></span></span><span class=line><span class=cl><span class=sd>    tolerations:
</span></span></span><span class=line><span class=cl><span class=sd>    - key: CriticalAddonsOnly
</span></span></span><span class=line><span class=cl><span class=sd>      operator: Exists
</span></span></span><span class=line><span class=cl><span class=sd>      effect: NoSchedule</span><span class=w>    
</span></span></span></code></pre></td></tr></table></div></div><p>Notice that I added the needed tolerations and node selector configuration to force Kubernetes to schedule the provider into the master node.</p><p>After a bit, K3S will deploy all the components of the provider, including a storage class called <code>nfs-storage</code>. This storage class is marked as default but since the pre-installed K3S storage class called <code>local-storage</code> is also marked as default there are 2 options:</p><ul><li>Delete the <code>local-storage</code> storage class</li><li>Edit the <code>local-storage</code> storage class to make it non default</li></ul><h4 id=traefik-custom-install>Traefik custom install</h4><p>The reason why I decided to modify the Traefik configuration is because by default, it won’t be able to get the real IP address of the clients because of the Traefik LoadBalancer configuration.</p><p>Before you ask, K3S use something called Klipper to create a load balancers inside the cluster. Normally, a load balancer is deployed outside but K3S do it this way to allow the usage of load balancer services easier.</p><p>Adding <code>externalTrafficPolicy: Local</code> (<a href=https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip target=_blank rel="noopener noreffer">More information</a>) to the spec section of the Traefik service will solve the problem but if I edit this option in the manifest K3S deploys, our changes won’t persist a reboot. Also I want to add a <code>nodeSelector</code> configuration to make sure Traefik is scheduled in the master node.</p><p>To achieve this little configuration change, we can create a file called <code>traefik-config.yaml</code> in the same location as the manifests:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>helm.cattle.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>HelmChartConfig</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>traefik</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>valuesContent</span><span class=p>:</span><span class=w> </span><span class=p>|-</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    service:
</span></span></span><span class=line><span class=cl><span class=sd>      spec:
</span></span></span><span class=line><span class=cl><span class=sd>        externalTrafficPolicy: Local
</span></span></span><span class=line><span class=cl><span class=sd>    nodeSelector:
</span></span></span><span class=line><span class=cl><span class=sd>      node-type: master</span><span class=w>    
</span></span></span></code></pre></td></tr></table></div></div><p>And why would I want to do all this may you ask? Why I need the real IP of the clients? Simple, for IP filtering. Using ingresses to access the services in the cluster is great so to expose something to the internet make sense to expose the Traefik LoadBalancer and make Traefik handle the requests. The problem of this approach is that anyone from the internet could reach private services what is not good.</p><p>With the configuration change I made, now I can create a Traefik middleware for all the ingresses I want to be private to prevent traffic from the internet to go to them:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>traefik.containo.us/v1alpha1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Middleware</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>private</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ipWhiteList</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>sourceRange</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>127.0.0.1</span><span class=l>/32</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>10.0.0.0</span><span class=l>/8</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>172.16.0.0</span><span class=l>/12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>192.168.0.0</span><span class=l>/16</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>The only thing left is asking Traefik to use it where I want. Just adding this to the metadata section of the Ingresses I don’t want to be accesible from the internet will do:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>traefik.ingress.kubernetes.io/router.middlewares</span><span class=p>:</span><span class=w> </span><span class=l>kube-system-private@kubernetescrd</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>Now the trafic to public services from the internet will pass through the ingress controller but if someone tries to get into a private service will get a Forbidden error message. This configuration is not the best for load balancing but is what I found to expose Traefik.</p><p>I added to the k3s-pi-cluster playbook a variable to change the <code>externalTrafficPolicy</code> option to <code>Local</code> or <code>Cluster</code> to let the playbook user decide what are its needs. Later I will explain a bit about how I ended up avoiding to expose Traefik to the internet and therefore using Traefik with the <code>externalTrafficPolicy</code> option set to <code>Cluster</code> as the default Trarfik configuration use.</p><h4 id=monitoring>Monitoring</h4><p>Since my idea is to have this cluster working 24/7 I need a monitoring solution that can alert me of something happens. I want to use the Prometheus, Alert Manager and Grafana stack for this (Prometheus will be used as metrics API instead of metric-server for thins like scaling pods).</p><p>The official repository doesn’t really support the installation on Raspberry Pi at the moment of writting but this repository do:</p><figure><a href=https://github.com/carlosedp/cluster-monitoring class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>GitHub - carlosedp/cluster-monitoring</div><div class=bookmark-description>The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances. This have been tested on a hybrid ARM64 / X84-64 Kubernetes cluster deployed as this article. This repository collects Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator.</div></div></div><img src=https://opengraph.githubassets.com/262a15cdb28fb73545a855c8c410a3f6e43d04186899dfa7dfdad20bc6e9ebb4/carlosedp/cluster-monitoring class=bookmark-image></a></figure><p>Cheers to the author because is awesome and works like a charm. Just follow the <code>README</code> file to install it in a K3S cluster. You want to follow the procedure in the master node, ensure openshift Python 3 library is installed.</p><p>Once everything is installed you should be able to acces Prometheus ,Grafana and Alert Manager. Just check the ingresses information to know where to access the services, this is how Grafana looks (Yeah the picture is one week after configuring all):</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/magi-project/images/grafana.png data-srcset="/posts/magi-project/images/grafana.png, /posts/magi-project/images/grafana.png 1.5x, /posts/magi-project/images/grafana.png 2x" data-sizes=auto alt=/posts/magi-project/images/grafana.png title=grafana width=1849 height=893></p><p>Since I don’t think that I will be using Prometheus and Alert Manager much I will just delete the ingresses to them and use kubectl port-forward to access them instead of having them open to the network.</p><p>I created a Telegram bot and configured Grafana to send alerts to me using it. It works incredibly well.</p><h4 id=network-policies>Network Policies</h4><p>Checking K3S documentation I noticed that the default <a href=https://github.com/containernetworking/cni target=_blank rel="noopener noreffer">CNI</a> was Flannel. Flannel is really cool because is pretty fast but is not able to handle network policies. I really want to limit the conections a pod can do, specially if the pod is accessed from the internet. I normally edit the iptables rules in my Raspberry Pi to handle this restrictions in Docker but since Kubernetes has network policies to handle this stuff I want to use them.</p><p>I researched my options here and I found this post about installing Canal:</p><figure><a href=https://dev.to/jmarhee/network-policies-with-canal-and-flannel-on-k3s-11oe class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>Network Policies with Canal and Flannel on K3s</div><div class=bookmark-description>Flannel is a popular Container Network Interface (CNI) addon for Kubernetes, however, it does not provide (because it is Layer 3 network focused on transport between hosts, rather than container networking with the host) robust support for NetworkPolicy resources. Now, the policy features from another popular CNI, Calico, can be imported to Flannel using Canal.</div></div></div><img src=https://dev.to/social_previews/article/944758.png class=bookmark-image></a></figure><p>The idea here is to keep using Flannel as CNI but install Calico as Network Policy manager. For installation just get the manifest <a href=https://docs.projectcalico.org/manifests/canal.yaml target=_blank rel="noopener noreffer">here</a>.</p><p>And search the environment variable called <code>CALICO_IPV4POOL_CIDR</code> that is commented. Since I’m installing Canal in K3S I need to uncomment the variable and set its value to <code>10.42.0.0/16</code>, what is the default Pod CIDR that K3S uses.</p><p>Just a quick note, during the installation the Canal pods were failing to start. The solution was to delete the <code>flannel.1</code> interface in every node with:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo ip link delete flannel.1
</span></span></code></pre></td></tr></table></div></div><p>The pods started without problems afterwards.</p><p>I don’t really know if it was a problem I had because of all the things I tested before the installation or something that need to be done when installing Canal and Flannel is already running.</p><h4 id=handling-certificates-and-application-security>Handling certificates and application security</h4><p>Last thing I need to finish the migration to Kubernetes is an easy way for issuing and managing TLS certificates and some kind of WAF. The solution I found to have this kind of stuff in Docker was this image:</p><figure><a href=https://hub.docker.com/r/linuxserver/swag class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>Docker Hub</div><div class=bookmark-description>SWAG - Secure Web Application Gateway (formerly known as letsencrypt, no relation to Let's Encrypt™) sets up an Nginx webserver and reverse proxy with php support and a built-in certbot client that automates free SSL server certificate generation and renewal processes (Let's Encrypt and ZeroSSL). It also contains fail2ban for intrusion prevention.</div></div></div><img src=https://hub.docker.com/favicon.ico class=bookmark-image></a></figure><p>Basically is Nginx, Certbot and Fail2ban working toguether. Kubernetes could work well with this solution using the <code>externalTrafficPolicy: Local</code> option in the Traefik load balancer to block the real IP address of the clients. The things is that, this is not something I want to use because doesn’t really scale well, the certificate management is pretty poor and I would need some kind of dashboard to really check what Fail2ban was doing.</p><p>For certificates the easier approach is to use cert-manager:</p><figure><a href=https://cert-manager.io/docs/ class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>cert-manager</div><div class=bookmark-description>cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. It can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI.</div></div></div><img src=https://cert-manager.io/images/cert-manager-logo-icon.svg class=bookmark-image></a></figure><p>The manifest need a little tweak to work with ARM, it is necessary to look for all the images used and add <code>-arm</code> at the end of all the image names. For example, if the image is <code>image:1.0</code> it is changed to <code>image-arm:1.0</code>. With that changes I was able to install cert-manager without problems.</p><p>Only one thing left for certificates, cert-manager need you to configure issuers to know how and where to ask for certificates. You can check how to create an Issuer in the cert-manager documentation.</p><p>I created an issuer with the <code>ClusterIssuer</code> kind instead of <code>Issuer</code>, that way the issuer would be able to work in all the namespaces, and Let&rsquo;s Encrypt as the CA to use. This issuer was configured to use a HTTP challenge in the port 80 that allows Let&rsquo;s Encrypt to verify that the domain is mine.</p><p>I will show you what changes to make to an ingress to ask for a certificates to the configured issuer:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>ISSUER-NAME</span><span class=w> </span><span class=c># This</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>app.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>service</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>port</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>number</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>pathType</span><span class=p>:</span><span class=w> </span><span class=l>Prefix</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>app.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>SECRET-NAME-FOR-CERT</span><span class=w> </span><span class=c># This</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>The secret name can be anything you want really but please, use a proper name to know what the secret contains.</p><p>I had some problems to make this work because I was using really strict network policies and I forgot to allow the cert-manager pod to have ingress connections to the port 80 for the http challenge.</p><p>If the certificate is issued correctly you should be able to see that the certificate is ready executing this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pi@raspberrypi:~ $ kubectl get certificate -n app
</span></span><span class=line><span class=cl>NAME                   READY   SECRET                 AGE
</span></span><span class=line><span class=cl>app-ingress-cert       True    app-ingress-cert       2s
</span></span></code></pre></td></tr></table></div></div><p>Also you should be able to see that the certificate is valid visiting the site. I only have a problem know, no Fail2ban in front of the services facing the internet.</p><p>The first thing I tried was to use Cloudflare proxying to filter the traffic. This works great but you should also filter the IPs that enter your network to only allow Cloudflare IPs to avoid attackers trying to bypass the Cloudflare protection. While checking for the Cloudflare IPs is when I found my favourite solution until now:</p><figure><a href=https://github.com/cloudflare/cloudflared class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>GitHub - cloudflare/cloudflared</div><div class=bookmark-description>Contains the command-line client for Cloudflare Tunnel, a tunneling daemon that proxies traffic from the Cloudflare network to your origins. This daemon sits between Cloudflare network and your origin (e.g. a webserver). Cloudflare attracts client requests and sends them to you via this daemon, without requiring you to poke holes on your firewall --- your origin can remain as closed as possible.</div></div></div><img src=https://opengraph.githubassets.com/51b16e5228ccc817a729b6456db19de921d5ccaa647faee0acf7c7768b6607f7/cloudflare/cloudflared class=bookmark-image></a></figure><p>This thing is just awesome, it allows you to create a tunnel between Cloudflare and you to avoid opening ports in the router or firewall. This actually fix another problem I have that I didn’t tell you about yet, my ISP blocks port 443 because apparently they use it for maintenance so for hosting the web services I have to use weird port numbers in the URLs. With Cloudflared the Clouldflare bypass and the port problems are fixed. Hosting a website in the internet without open ports, ideal.</p><p>To use this system, the only thing really needed is a Cloudflare account and a domain. The rest is to follow the documentation to set it up correctly. I created a little Helm chart that will help if some of you wants to use this approach:</p><figure><a href=https://github.com/anthares101/k3s-pi-cluster-charts/tree/master/cloudflared class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>GitHub - anthares101/k3s-pi-cluster-charts/cloudflared</div><div class=bookmark-description>A place for my Helm charts! Contribute to anthares101/k3s-pi-cluster-charts development by creating an account on GitHub.</div></div></div><img src=https://opengraph.githubassets.com/7044e1dfe81c940e6e18136248081ba4e4d4cee21d021591181bb7cdd0bf25b1/anthares101/k3s-pi-cluster-charts class=bookmark-image></a></figure><p>I almost forgot, remember the HTTP challenge I used for getting the certificates with cert-manager? Well if I close the router ports obviously this stops working, there is a solution though. Let&rsquo;s Encrypt allows another method for validation and it is called <code>dns01</code> in cert-manager:</p><figure><a href=https://cert-manager.io/docs/configuration/acme/dns01/cloudflare/ class="bookmark source" target=_blank><div class=bookmark-info><div class=bookmark-text><div class=bookmark-title>Cloudflare</div><div class=bookmark-description>To use Cloudflare, you may use one of two types of tokens. API Tokens allow application-scoped keys bound to specific zones and permissions, while API Keys are globally-scoped keys that carry the same permissions as your account. API Tokens are recommended for higher security, since they have more restrictive permissions and are more easily revocable.</div></div></div><img src=https://cert-manager.io/images/cert-manager-logo-icon.svg class=bookmark-image></a></figure><p>It is true that is not as easy to setup as the HTTP challenge but with this new approach that is just the way to go.</p><h2 id=some-tips>Some tips</h2><h3 id=importing-images-to-k3s>Importing images to K3S</h3><p>Trying to migrate an application I had that uses an image that was not in Dockerhub made me discover that it is possible to just import images to the K3S image store:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Export an image from your Docker image store to a tar file</span>
</span></span><span class=line><span class=cl>docker save --output test-app-v1.0.0.tar test-app:v1.0.0
</span></span><span class=line><span class=cl><span class=c1># Import an image to the K3S image store in the node</span>
</span></span><span class=line><span class=cl>sudo k3s ctr images import /home/ubuntu/test-app-v1.0.0.tar
</span></span></code></pre></td></tr></table></div></div><p>The only bad thing is that, each node have its own image store so you have to import the image too all of them in order to avoid problems with scheduling.</p><p>I guess a much better solution could be to host my own image registry in the cluster o my Raspberry Pi but this works too.</p><h3 id=k3s-snapshots>K3S snapshots</h3><p>Using the default K3S database, sqlite, makes the creation of snapshots really simple. Just stop the K3S service and copy the entire <code>/var/lib/rancher/k3s/server</code> directory for restoration (Obviously after the copy, start the K3S service again).</p><h2 id=last-thoughts>Last Thoughts</h2><p>The journey was pretty long and I really had lot of problems to get some things exactly as I wanted. But now that everything is working I can say that the migration was really a success, my old Raspberry Pi can now just focus on essential applications that support my internal network like the Samba, Pihole and NFS servers and the cluster will handle all the rest of the stuff including all those services that I want to expose to the internet.</p><p>The management of exposed services with Kubernetes is much simplier using network policies and cert-manager and the addition of Cloudflared (I know I could use it in Docker too but let me be happy with it) fixing all the problems with the ports and giving me a WAF with a fancy dashboards is just perfect.</p><p>PD: Someone noticed the Evangelion references?</p><style>.source{border:1px solid #ddd;border-radius:3px;padding:1.5em;word-break:break-all}figure{margin:1.25em 0;page-break-inside:avoid}.icon{display:inline-block;max-width:1.2em;max-height:1.2em;text-decoration:none;vertical-align:text-bottom;margin-right:.5em;margin-top:.1em}.bookmark{text-decoration:none;max-height:8em;padding:0;display:flex;width:100%;align-items:stretch}.bookmark-title{font-size:.85em;overflow:hidden;text-overflow:ellipsis;height:1.75em;white-space:nowrap}.bookmark-text{display:flex;flex-direction:column;text-align:left}.bookmark-info{flex:4 1 180px;padding:12px 14px 14px;display:flex;flex-direction:column;justify-content:space-between}.bookmark-image{width:33%;flex:1 1 180px;display:block;position:relative;object-fit:cover;border-radius:1px}.bookmark-description{font-size:.65em;overflow:hidden;max-height:4.5em;word-break:break-word}.bookmark-href{font-size:.75em;margin-top:.25em;width:50%;display:flex}</style></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-08-09</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://blog.anthares101.com/posts/magi-project/ data-title="MAGI Project" data-via=@Anthares101 data-hashtags="kubernetes,blue team,homelab"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://blog.anthares101.com/posts/magi-project/ data-hashtag=kubernetes><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://blog.anthares101.com/posts/magi-project/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://blog.anthares101.com/posts/magi-project/ data-title="MAGI Project"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://blog.anthares101.com/posts/magi-project/ data-title="MAGI Project"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://blog.anthares101.com/posts/magi-project/ data-title="MAGI Project" data-image=images/Assembly3.jpg><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/kubernetes/>kubernetes</a>,&nbsp;<a href=/tags/blue-team/>blue team</a>,&nbsp;<a href=/tags/homelab/>homelab</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/posts/ target=_blank>Anthares101</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"No results found",snippetLength:30}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>