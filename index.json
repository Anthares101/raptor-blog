[{"categories":["Infrastructure"],"content":"Write up about the process of creating my personal Kubernetes cluster","date":"2022-02-09","objectID":"/magi-project/","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"MAGI project ","date":"2022-02-09","objectID":"/magi-project/:0:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"The idea I have been thinking about the idea of building a cluster made of Raspberrys and update my home infrastructure a bit. Right now, I use a Raspberry Pi 4 B as a little server to run Pi Hole, personal projects, Plex… and also as a NAS. All this sevices are run using Docker containers because I love Docker and the management is easier that way, allowing me to recover from a failure really quick. Since I’m increasing the load to it with more things and some application like Plex sometimes consumes A LOT a cluster could be awesome. After thinking a bit my options I discovered PicoCluster: PicoCluster - Desktop Micro Data Center Kubernetes Docker Cluster Software Advanced Kits and Assembled Cubes come with 4GB Raspberry 4 boards. Starter Kits support 1GB, 2GB and 4GB boards. They have a lot of cool things and one of them is exactly what I needed, a little cluster with 3 Raspberrys. I bought it, if it is not obvious by now, and the plan is to migrate some of the things I have running in my actual Pi to it. The idea is simple, my initial Raspberry will only have essential network services like Pi Hole (Working as DHCP and DNS server), Samba (Used for backups and to add things to Plex) and Netdata for monitoring it 24/7 and alert me if something is going wrong. Meanwhile, the cluster will use that Pi for storage provisioning and will host the rest of application like Plex, Nextcloud, personal projects… Also, as part of this project I want to make sure I can still recover from a terminal failure without a ton of problems and learn a bit more of Ansible. The idea here is that all the cluster initial installation and setup will be performed by an Ansible Playbook that I will create from scratch. ","date":"2022-02-09","objectID":"/magi-project/:1:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"What are you going to read Anthares from the future here! The initial idea sounds great right? A cluster of Raspberrys what a cool thing! Well, looks like migrating all to Kubernetes from a Docker setup is not as straightforward as I thought. Also, while I was working on it I started thinking about improving the security of the new infrastructure and fix some problems I had taking advantage of some Kubernetes features. What I though It was going to be a fast and smooth process (I already have worked with Kubernetes and Helm before) ended up being a really long journey of reading documentation and unexpected problems BUT I really learnt a lot. I tried to cover all the process I followed during my journey so I invite you to relax, get a coffee or something and join me in this adventure, Its going to be a long one. Keep in mind that this is not a Kubernetes tutorial, if you want to follow my steps you are supposed to already know a bit about how things works in Kubernetes. Oh! I almost forgot about it. Keep in mind that the 64 bit version of Raspbian was just released yesterday, 02/04/2022, so obviously during the installation process of this write up I had to install a pre-release version of it. ","date":"2022-02-09","objectID":"/magi-project/:1:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Assembly After waiting a bit (I’m in Spain so it is a long way from the US) a received my order! You can ask them to do the assembly process or even install the applications you want into the cluster to avoid wasting time setting it up but… that is not fun right? The assembly process is documented fairly well in their site and you will only need a screwdriver and some different size heads for it in order to follow all the process. By the way, you can ask them to only send the parts for the cluster but not the actual boards for it. The thing is that with the chip shortage we have right now the price they sell them for is just too good to not get all from them. The assembly took me around 2 hours or so but it was pretty fun to do and it looks incredible: ","date":"2022-02-09","objectID":"/magi-project/:2:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Preparing the SD cards We have a cool looking cube right now, time to bring it to life! The first thing to do is to prepare the SD cards with Raspbian and a basic headless setup (No monitors please). In this case I want to use the 64 bit version of Raspbian (Even though it is still under development and have some issues) because since Apple started using ARM a lot of applications work with ARM64 and can be handy. This Raspbian version can be downloaded from here. I will use the Raspberry Pi imager program to put Raspbian in the SD cards. You can find this software here: Raspberry Pi OS - Raspberry PiFrom industries large and small, to the kitchen table tinkerer, to the classroom coder, we make computing accessible and affordable for everybody. There is an option in the program for custom images that works like a charm: It will take a while just to prepare one card so imagine 3 of them. Once the cards are prepared, we have to do one more thing. Since we don’t want to use a monitor we have to connect the cards to the computer, open the disk called boot and create a file called ssh . This will enable SSH by default. The only problem right now is that the IP address for each Raspberry Pi will be provided by the DHCP server so in the first boot you will need to find them in the network. ","date":"2022-02-09","objectID":"/magi-project/:3:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Initial setup My idea is to create an ansible playbook to make all the initial setup of the cluster, that way if a terminal failure happens I can just get everything setup in the blink of an eye. But as I said above, the Raspberrys will have a random IP in the first boot so I will have to connect to them one by one to setup an static IP address and hostnames. After that, I can add SSH key authentication and from there I can start with the playbook to setup everything. I know it is not perfect because if something goes wrong and I need to reinstall from a clean SD card, I will need to do this whole process of connecting to the Raspberry and make the very first setup by hand before I can use the playbook but I think it is ok for me. Note: I had an error with the 64 bit Raspbian OS about the locale value. I just executed sudo dpkg-reconfigure locales and generated the language that the error was crying about. ","date":"2022-02-09","objectID":"/magi-project/:4:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Cluster setup All the steps described in this section will be included in an Ansible playbook to make this whole process automatic. You can check the playbook here: GitHub - anthares101/k3s-pi-cluster: K3S Pi Cluster project playbookThe monitoring stack used is the Carlos Eduardo version of the kube-prometheus repo and part of the Ansible roles were adapted from Jeff Geerling turing-pi-cluster project. ","date":"2022-02-09","objectID":"/magi-project/:5:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Basic Raspberrys setup Raspbian comes with some stuff configured that we don’t really need, Wi-Fi and Bluetooth for example. To disable them we can just add this to the /boot/config.txt file and reboot: dtoverlay=pi3-disable-wifi dtoverlay=pi3-disable-bt The next thing is to prevent the default pi user from using the sudo command without a password. This is easy, just delete /etc/sudoers.d/010_pi-nopasswd . Also, it would be awesome if the Raspberrys date is correct so set the correct timezone: sudo timedatectl set-timezone \u003cyour_time_zone\u003e For my last trick, I will make some changes to harden the system a bit. The home directory of the pi user is world readable so let’s change that: chmod 0750 /home/pi And since we added a SSH key to the Raspberrys for SSH authentication I will disable the access through SSH using password and also won’t allow root user login. Adding this lines to /etc/ssh/sshd_config and reloading the sshd service will do: PermitRootLogin no UsePAM no PasswordAuthentication no ","date":"2022-02-09","objectID":"/magi-project/:5:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Installing Kubernetes The Kubernetes version I will be installing is K3S: Lightweight KubernetesThe above figure shows the difference between K3s server and K3s agent nodes. For more information, see the architecture documentation. We are a Cloud Native Computing Foundation sandbox project. Should include all the normal Kubernetes features but in a more optimized way. The installation is pretty easy to do actually, it is documented very well in the K3S page and the only thing you have to do is to run an installation script in each node of the cluster. The only thing to keep in mind is that you have to start with the installation of the master node (By default K3S won’t allow you to have more than one) to get a secret token you need for the worker nodes installation. By default the K3S script will deploy some components to the cluster, for a basic testing environment this is ok but if you want to customize this components configuration you need to tell K3S to not deploy them to avoid getting your configuration getting overwrited in every reboot. This page shows what parameters to use in order to customize the installation: K3s Server Configuration ReferenceIn this section, you'll learn how to configure the K3s server. Throughout the K3s documentation, you will see some options that can be passed in as both command flags and environment variables. For help with passing in options, refer to How to Use Flags and Environment Variables. Before starting with the installation I will add this parameters into the /boot/cmdline.txt file of every Pi to make sure the containers work as expected: cgroup_memory=1 cgroup_enable=memory Master node Now we are ready to install K3S in the master node. Since I want to use Prometheus to get the metrics from the nodes instead of the typical Kubernetes metric-server I will use -no-deploy metrics-server to tell K3S to not deploy this component: ./k3s_install.sh --no-deploy metrics-server Once this finish, make sure you get the token for the workers nodes: /var/lib/rancher/k3s/server/node-token and also the kubeconfig file to be able to manage your cluster: /etc/rancher/k3s/k3s.yaml. Worker nodes To add you workers node to the cluster just execute this: K3S_URL=https://\u003cMASTER-NODE-IP\u003e:6443 K3S_TOKEN=\u003cTOKEN\u003e ./k3s_install.sh ","date":"2022-02-09","objectID":"/magi-project/:5:2","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Storage provisioning For persistent volume provisioning I don’t really want to rely in the local path provider that K3S use by default. The problem with that provider is that if a pod is re-scheduled in a different node it won’t be able to access the persistent volume data because that volume is in another node. In order to avoid this problems, I will prepare a NFS server in the original Raspberry Pi I talked you about before and configure a NFS provider in the cluster that points to that server. The NFS server could be in one of the cluster nodes aswell if you don’t have another host to use. NFS server installation To be honest the configuration of the a NFS server was easier than I expected. Just install nfs-kernel-server package and prepare a share folder to use. In the /etc/exports is where you have to configure the share for NFS and also the IPs addreses that can access it. I will let here a little example: /home/pi/Shared MASTER_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) WORKER1_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) WORKER2_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) Make sure to run sudo exportfs -ra to update NFS exports and also to check that nfs-server and rpc-statd.service are running. ","date":"2022-02-09","objectID":"/magi-project/:5:3","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Cluster configuration Taints and labels So we have a Kubernetes cluster but it needs a bit of tweaking First of all, since I have 3 nodes I want to taint and label the master node to control what applications can be scheduled to it: kubectl taint nodes melchior CriticalAddonsOnly:NoSchedule kubectl label node melchior node-type=master I used that taint because I noticed that all the addons (That is the name K3S use for all the manifests that are put in the /var/lib/rancher/k3s/server/manifests folder for deployment) deployed by K3S use that toleration. The label is one I invented. Tainting the master node will give me control over what pods are scheduled into the master node. The idea is to only allow critical application to be in the master node, this way the master node will be more protected against resource intensive applications that can cause a node to crash. NFS storage provider I will use this NFS provider: GitHub - kubernetes-sigs/nfs-subdir-external-provisionerNFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}. Note: This repository is migrated from https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client. Since it offers the installation through Helm and K3S accepts Helm as a way of deploying an addon, just copying this file to /var/lib/rancher/k3s/server/manifests will be enough: --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: nfs-storage namespace: kube-system spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace: kube-system set: nfs.server: NFS-SERVER-IP nfs.path: NFS-SHARE-PATH storageClass.name: nfs-storage storageClass.accessModes: ReadWriteMany storageClass.reclaimPolicy: Retain storageClass.archiveOnDelete: \"false\" storageClass.defaultClass: \"true\" valuesContent: |- nodeSelector: node-type: master tolerations: - key: CriticalAddonsOnly operator: Exists effect: NoSchedule Notice that I added the needed tolerations and node selector configuration to force Kubernetes to schedule the provider into the master node. After a bit, K3S will deploy all the components of the provider, including a storage class called nfs-storage. This storage class is marked as default but since the pre-installed K3S storage class called local-storage is also marked as default there are 2 options: Delete the local-storage storage class Edit the local-storage storage class to make it non default Traefik custom install The reason why I decided to modify the Traefik configuration is because by default, it won’t be able to get the real IP address of the clients because of the Traefik LoadBalancer configuration. Before you ask, K3S use something called Klipper to create a load balancers inside the cluster. Normally, a load balancer is deployed outside but K3S do it this way to allow the usage of load balancer services easier. Adding externalTrafficPolicy: Local (More information) to the spec section of the Traefik service will solve the problem but if I edit this option in the manifest K3S deploys, our changes won’t persist a reboot. Also I want to add a nodeSelector configuration to make sure Traefik is scheduled in the master node. To achieve this little configuration change, we can create a file called traefik-config.yaml in the same location as the manifests: apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: traefik namespace: kube-system spec: valuesContent: |- service: spec: externalTrafficPolicy: Local nodeSelector: node-type: master And why would I want to do all this may you ask? Why I need the real IP of the clients? Simple, for IP filtering. Using ingresses to access the services in the cluster is great so to expose something to the internet make sense to expose th","date":"2022-02-09","objectID":"/magi-project/:5:4","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Some tips ","date":"2022-02-09","objectID":"/magi-project/:6:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Importing images to K3S Trying to migrate an application I had that uses an image that was not in Dockerhub made me discover that it is possible to just import images to the K3S image store: # Export an image from your Docker image store to a tar file docker save --output test-app-v1.0.0.tar test-app:v1.0.0 # Import an image to the K3S image store in the node sudo k3s ctr images import /home/ubuntu/test-app-v1.0.0.tar The only bad thing is that, each node have its own image store so you have to import the image too all of them in order to avoid problems with scheduling. I guess a much better solution could be to host my own image registry in the cluster o my Raspberry Pi but this works too. ","date":"2022-02-09","objectID":"/magi-project/:6:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"K3S snapshots Using the default K3S database, sqlite, makes the creation of snapshots really simple. Just stop the K3S service and copy the entire /var/lib/rancher/k3s/server directory for restoration (Obviously after the copy, start the K3S service again). ","date":"2022-02-09","objectID":"/magi-project/:6:2","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Last Thoughts The journey was pretty long and I really had lot of problems to get some things exactly as I wanted. But now that everything is working I can say that the migration was really a success, my old Raspberry Pi can now just focus on essential applications that support my internal network like the Samba, Pihole and NFS servers and the cluster will handle all the rest of the stuff including all those services that I want to expose to the internet. The management of exposed services with Kubernetes is much simplier using network policies and cert-manager and the addition of Cloudflared (I know I could use it in Docker too but let me be happy with it) fixing all the problems with the ports and giving me a WAF with a fancy dashboards is just perfect. PD: Someone noticed the Evangelion references? ","date":"2022-02-09","objectID":"/magi-project/:7:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"}]