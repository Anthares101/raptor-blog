[{"categories":["Offensive Cybersecurity"],"content":"Post about how to setup the Flipper Zero in order to perform different Wi-Fi attacks","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"How to setup the Flipper Zero for Wi-Fi attacks Before starting, remember that performing any of the attacks explained in this post to networks without previous consent is illegal, this post information is for educational purposes only. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:0:0","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Introduction You may be thinking: “I just buy the Wi-Fi Flipper board and call it a day” well, not really. Even buying the Flipper Wi-Fi Devboard instead of making a custom module won’t enable the Flipper to start de-authenticating people out of the box, you will need to flash the proper firmware. This post will focus on the Flipper Wi-Fi Devboard but, in theory, the process described should work with anything based on the ESP32-S2 module that can be connected to the Flipper. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:1:0","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Requisites Obviously a Flipper Zero. A Flipper Wi-Fi Devboard. Any device that can be use to transfer necessary files to the Flipper. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:2:0","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Go time! ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:3:0","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Preparing necessary files The Wi-Fi Devboard comes with a firmware call Black Magic flashed, this is enough for updating the Flipper through Internet or in-circuit debugging but not really to play with Wi-Fi networks. 2024-02-19: If you are reading this, from Marauder v0.13.5 the firmware allows Evil Portal HTML files to be pulled from Flipper Zero SD and the PCAP problem is solved. My recommendation is to only follow the steps below related with Marauder and ignore the alternative firmware used only for Evil Portal compatibility. Make sure to install the last Marauder version! They removed the _sd_serial.bin firmware, just intall the Flipper one. After looking around a bit, looks like flashing Marauder firmware should provide all the functionality needed. The only problem here is that this firmware is obviously designed for this tool and looks like certain features are not really working with the Flipper Wi-Fi Devboard. After some testing, the last firmware version that properly enable the Flipper to sniff Wi-Fi frames and safe PCAP files without problems is the v0.11.0-RC3 (At the time of writing this post). Newer versions result in corrupted PCAP files for some reason. Also, looks like the evil portal functionality is only available by modifying the Wi-Fi Devboard to add a SD card slot to it. In theory, these limitations will be fixed in the future but for now, it is possible to use an alternative firmware to circumvent the problem. Luckily it is possible to flash two different firmwares to the Flipper Wi-Fi Devboard and change between them, so here it is what is needed: Marauder firmware v0.11.0-RC3, the file ending in _sd_serial.bin is the right one. Go to the Marauder wiki and get the proper bootloader and partitions files. The Flipper Marauder app is called [ESP32] WiFi Marauder and can be installed from the Flipper application hub. Last version of the evil portal firmware, get the files called evil_portal_sd_folder.zip and wifi_dev_board.zip and unzip them. The Flipper app for the Evil Portal can be downloaded from here. Make sure to download the app for the proper Flipper firmware version. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:3:1","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Copying files to the Flipper The idea now is to take the downloaded stuff and organize it properly so the Flipper is able to use them. Let’s start with the firmwares, first create a directory called esp_flasher and inside it prepare the next structure: EvilPortal EvilPortal.ino.bootloader.bin EvilPortal.ino.partitions.bin EvilPortal.ino.bin Marauder esp32_marauder.ino.bootloader.bin esp32_marauder.ino.partitions.bin esp32_marauder_*_flipper_sd_serial.bin boot_app0.bin Next, get the contents of the evil_portal_sd_folder.zip and the esp_flasher directory created before and copy both directories to apps_data inside the Flipper SD card. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:3:2","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Flashing The flashing process will be done through the Flipper using the app called ESP Flasher, can be installed through the application hub. Since all the files are already prepared the process is pretty easy, just make sure to put the Flipper Wi-Fi Devboard in flashing mode: Once the board is connected to the Flipper, hold the boot button (Right button) and press the reset button. Go into the flashing application in the Flipper and select Switch to Firmware A, if the board is in flashing mode no errors should appear on screen. Once that is finished, put the board again in flashing mode and this time choose the Flash ESP option. Setup these fields: Bootloader: Marauder/esp32_marauder.ino.bootloader.bin Part Table: Marauder/esp32_marauder.ino.partitions.bin boot_app0: boot_app0.bin FirmwareA: Marauder/esp32_marauder_*_flipper_sd_serial.bin FirmwareB: EvilPortal/EvilPortal.ino.bin Hit FLASH and wait. After some time, the process should complete successfully and only one last step is needed. Right now only the partitions for the Marauder firmware are flashed into the board so the EvilPortal partitions need to be flashed aswell. Go to the flashing application, select Switch to Firmware B and then go again into the Flash ESP option (Remember to put the board into flashing mode everytime!): Part Table: EvilPortal/EvilPortal.ino.partitions.bin Hit FLASH and wait. You may have noticed that the EvilPortal/EvilPortal.ino.bootloader.bin was not flashed, well only one bootloader can be flashed into the board at once. Luckily both firmware bootloaders are similar so, at least at the time of writing, this flashing process should be enough. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:3:3","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Offensive Cybersecurity"],"content":"Last thoughts Once the flashing process is complete everything should be good to go. Switching between the Marauder firmware (Firmware A) and the Evil Portal firmware (Firmware B) can be achieved through the ESP Flasher app so it is possible to use both the [ESP32] WiFi Marauder and [ESP32] Evil Portal apps without any problems using the proper firmware. The part of configuring an evil portal or using the Marauder app is not covered here. Just go to their repective documentation pages and take a look, the hard part is already done I promise! NOTE: If someone wants to know a bit more about how the ESP32 chip works, this StackOverflow question is awesome. ","date":"2023-09-16","objectID":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/:4:0","tags":["wifi","red team","network"],"title":"How to setup the Flipper Zero for Wi-Fi attacks","uri":"/how-to-setup-the-flipper-zero-for-wi-fi-attacks/"},{"categories":["Infrastructure"],"content":"Write up about how MAGI has behaved in the last year and how I managed to keep it alive","date":"2023-05-08","objectID":"/magi-project-a-year-later/","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"MAGI Project a year later How has MAGI behaved in the last year? Is the Kubernetes cluster even alive? Well, the header image is kind of spoiling the answers to those questions but, since the idea of the post is talking a bit about the implication of having an on premise Kubernetes cluster, you may be interested in continue reading. ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:0:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"Is MAGI still running? You probably already knew the answer but yes! Is still running. All the nodes are healthy and running up to date, I have some applications running and everything is pretty smooth right now. The maintenance of the cluster was not the easier at the begining but I was able to ease the process a lot with scripts and Ansible. Later in the post we will talk a bit about that. ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:1:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"How about the availability? This is another of the questions you may have, the cluster is alive and running fine now but during all this time was always the case? Obviusly no, I had some downtimes caused by typical on premise problems: blackouts, problems during updates, crashing applications, hardware failures… But I have to say the uptime is not that bad, you can actually check that in: https://status.anthares101.com/, around 99.9% after a year (Both my write ups page and this blogs are hosted on GitHub so ignore those two). Of course the availability of VPS in a cloud provider is far better and you dont have to deal with hardware stuff but what is the fun of that? ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:2:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"Maintenance is key If you want a system to run for a long time, you need maintenance. I knew that at the time so that is why I decided to make the process as painless as possible, manually updating systems and applications or creating backups by hand… too much work, so here is how I did it. ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:3:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"Updates Docker images The cluster is not the only asset I have as you may know, for the cluster storage I use a fourth Raspberry Pi that also host different local services in my network using Docker containers. To make sure all the applications running in Docker get updated regularly, I use Watchtower, it is really easy to use and will make sure every container is running with the latest image once it is setup. Kubernetes images For the cluster I needed something similar, Watchtower is not an option to keep Kubernetes pods running with the latest images so I started looking around for alternatives. I came up with Keel, a Kubernetes operator to automate Helm, Deployments, StatefulSet and DaemonSet updates. The process of installing it is pretty easy, the problem I had is that the official repository only build images for x86 systems and not ARM. Since the official repository support seems… gone, I created a fork of it and using the magic of GitHub actions I automated the process of pulling new changes from the official repository to keep the fork updated and also automated the build and push of the Keel image for both ARM and x86 to DockerHub so I could use it without problems in my cluster. Once I had all running, nearly all the images in the cluster are managed by Keel so I don’t need to update them one by one. K3S and Linux Well I created a little Ansible playbook to update all the cluster nodes OS, the K3S version and also the monitoring stack. You can find the playbook here. Using that and a little script, I’m able to update everything in my home lab! #! /bin/bash # raspi-maintenance echo \"Updating cluster nodes, K3S and monitoring stack...\" (cd ~/Cluster/k3s-pi-cluster-upgrade \u0026\u0026 ansible-playbook main.yaml -K) echo \"Updating this system...\" sudo apt update \u0026\u0026 sudo apt dist-upgrade -y if [ -f /var/run/reboot-required ]; then echo 'System updated! Reboot required...' sudo reboot fi echo 'All updated!' The script is run from the fourth Raspberry Pi I mentioned earlier. ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:3:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"BackUps I also created a little script to backup important stuff to my disks array (My fourth Pi manage a RAID1 array for storage) to make sure I have everything in a single place. This allows me to just launch a single rsync command to sync everything important in my disk array with my cold backup. #! /bin/bash # raspi-backup YELLOW='\\033[0;33m' NC='\\033[0m' # No Color echo -e \"${YELLOW}K3S backup in progress...${NC}\" K3S_BACKUP_NAME=\"k3s-backup.tar.gz\" ssh -t melchior \"sudo tar -czf /tmp/$K3S_BACKUP_NAME /var/lib/rancher/k3s/server \u0026\u0026 sudo chown pi:pi /tmp/$K3S_BACKUP_NAME\" scp melchior:/tmp/$K3S_BACKUP_NAME ~/Raid/Private/Anthares/Vault/Backups/RaspberryBackups/$K3S_BACKUP_NAME ssh melchior rm -f /tmp/$K3S_BACKUP_NAME echo -e \"${YELLOW}Home backup in progress...${NC}\" HOME_BACKUP_NAME=\"home-backup.tar.gz\" tar -czf ~/Raid/Private/Anthares/Vault/Backups/RaspberryBackups/$HOME_BACKUP_NAME ~/.ssh ~/.kube ~/Raspi-opt ~/Cluster ~/Docker echo -e \"${YELLOW}Docker volumes backup in progress...${NC}\" DOCKER_VOLUMES_BACKUP_NAME=\"docker-volumes-backup.tar.gz\" sudo tar -czf ~/Raid/Private/Anthares/Vault/Backups/RaspberryBackups/$DOCKER_VOLUMES_BACKUP_NAME /var/lib/docker/volumes/ sudo chown pi:pi ~/Raid/Private/Anthares/Vault/Backups/RaspberryBackups/$DOCKER_VOLUMES_BACKUP_NAME It is not perfect but it works fine for me. Again, this script is run from my fourth Raspberry Pi. ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:3:2","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Infrastructure"],"content":"Last thoughs The process of thinking about how to make sure everything was automated was long. I had to install stuff, build all these scripts and playbooks… but at the end it was worth it. Before having all this ready, I could spent hours updating things here an there, making sure I had backups for pretty much everything… But now I can simply use two commands (maybe three if you count the rsync one) from my fourth Pi a call it a day: raspi-maintenance raspi-backup Probably, the cluster would not have survived until now without all this process to ease the maintenance of it but once nearly everything is automated? As long as the hardware does not die, I will make sure the cluster stays alive for a long time. The importance of automation guys! ","date":"2023-05-08","objectID":"/magi-project-a-year-later/:4:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project a year later","uri":"/magi-project-a-year-later/"},{"categories":["Offensive Cybersecurity"],"content":"Post about how we can route all our traffic through the DNS protocol to bypass certain protections","date":"2022-11-13","objectID":"/dns-tunneling/","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"DNS tunneling The idea of this technique is to use the DNS protocol to route all the traffic, just like a VPN! Of course this method will make your connection slower but will allow you to bypass certain systems like network filters or captive portals. Obviously, you could use this to exfiltrate data too. The reason why this works is that normally when captive portals or firewalls rules are setup, the administrators block all TCP and UDP connections when certain rule is met but they forget about checking DNS queries. This means that, if all our traffic is routed through DNS, we will be able to bypass all this protections. In this article, I will explain two ways of setup: The first one will make sure the tunnel is encrypted like a proper VPN using SSH, easier to setup but only usable on computers or rooted phones. The second is a bit harder to setup but you will be able to use it in both non rooted phones and computers (No encryption though). ","date":"2022-11-13","objectID":"/dns-tunneling/:0:0","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"Requisites A VPS or machine that can be reached from the Internet for DNS requests without problems. This little tool installed both in the client and the server that will handle the tunnel: Iodine. Obviously a Domain. ","date":"2022-11-13","objectID":"/dns-tunneling/:1:0","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"Server setup First of all we need to setup our VPS to be the authoritative DNS for one of our domains or subdomains (I prefer the second option to be honest). The procedure to do this looks like it is different from vendor to vendor so I will focus on Cloudflare. Setup in one of our domains a NS record for a subdomain or our whole domain. In my case I will setup dns.test.com NS record to point to my VPS vps.francecentral.cloudapp.azure.com. This will tell Cloudflare that now, the authoritative DNS server is our VPS so the DNS queries should be sent to it. Make sure you point your NS record to an A record, at least in Cloudflare this is necessary and the record creation will fail if you try to use an IP address or a CNAME. With that out of the way, go to your server and install Iodine (You can do it through apt). Now execute this (Make sure port 53 TCP/UDP is available): raptor@kestrel:~$ sudo iodined -f 10.2.0.1 dns.test.com Enter tunnel password: Opened dns0 Setting IP of dns0 to 10.2.0.1 Setting MTU of dns0 to 1130 Opened IPv4 UDP socket Opened IPv6 UDP socket Listening to dns for domain dns.test.com This will start the tool in server mode, the 10.2.0.1 IP is just the /27 CIDR block that the tunnel will use, for example: SERVER ------------------------------------ CLIENT 10.2.0.1 10.2.0.2 Make sure that the IP block used by the tunnel is not already in use by any of your client/server adapters. The password it is used to authenticate the clients and avoid unknown users. ","date":"2022-11-13","objectID":"/dns-tunneling/:2:0","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"Troubleshooting Port 53 TCP/UDP has to be reachable and obviouly useable by Iodine. If you have properly setup the network rules for your VPS to allow traffic to this port but you are still having problems, maybe you should check iptables configuration. ","date":"2022-11-13","objectID":"/dns-tunneling/:2:1","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"Client setup Once the Server is prepared, let’s connect to it. In our machine we will need Iodine too, luckily Kali already has it. Just execute this (The password is the one you configured in the server earlier): ┌──(kali㉿kali)-[~] └─$ sudo iodine -f -r vps.francecentral.cloudapp.azure.com dns.test.com Enter password: Opened dns0 Opened IPv4 UDP socket Sending DNS queries for dns.test.com to REDACTED Autodetecting DNS query type (use -T to override). Using DNS type NULL queries Version ok, both using protocol v 0x00000502. You are user #0 Setting IP of dns0 to 10.2.0.2 Setting MTU of dns0 to 1130 Server tunnel IP is 10.2.0.1 Skipping raw mode Using EDNS0 extension Switching upstream to codec Base128 Server switched upstream to codec Base128 No alternative downstream codec available, using default (Raw) Switching to lazy mode for low-latency Server switched to lazy mode Autoprobing max downstream fragment size... (skip with -m fragsize) 768 ok.. 1152 ok.. ...1344 not ok.. ...1248 not ok.. ...1200 not ok.. 1176 ok.. 1188 ok.. will use 1188-2=1186 Setting downstream fragment size to max 1186... Connection setup complete, transmitting data. The output should be like that if everything went ok. The vps.francecentral.cloudapp.azure.com part of the command is optional, basically you can specify the nameserver you want to use, if your server is reachable put it here to improve speed. Verify that the tunnel is open by going to a terminal and checking ifconfig, if you have the adapter dns0 all is good: ┌──(kali㉿kali)-[~] └─$ ifconfig dns0 dns0: flags=4305\u003cUP,POINTOPOINT,RUNNING,NOARP,MULTICAST\u003e mtu 1130 inet 10.2.0.2 netmask 255.255.255.224 destination 10.2.0.2 unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 txqueuelen 500 (UNSPEC) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1 bytes 48 (48.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Now we can setup a SOCKS5 proxy and start routing our traffic, I recommend to use SSH for this to make sure the traffic is encrypted, otherwise the traffic can be intercepted and read if using insecure protocols (Remember that DNS is not encypted): ┌──(kali㉿kali)-[~] └─$ ssh -D 8080 -q -C -N raptor@10.2.0.1 -i Projects/dns-tunnel/raptor.pem Some notes about this command: The IP address used: 10.2.0.1 is our VPS at the end of our DNS tunnel The -D option is just telling SSH where the SOCKS5 proxy will listen for requests in our machine The -q is just for quiet operation The -N will tell SSH that we don’t want to execute commands so no prompt And the most important option, -C. This will compress the traffic to reduce the bandwidth, what is something you are gonna be thankful about since the DNS tunnel is not what you will call something fast. Now just tunnel your traffic through the proxy! You can use something like proxychains or the OS proxy configuration to start tunneling stuff through the tunnel. In my case I will just setup Mozilla to use the proxy. I love FoxyProxy so let’s setup the proxy there: My VPS is located in Paris and as you can see the traffic is being routed from there: Also check this Wireshark capture, weird DNS queries everywhere! This is just Iodine encoding all our packages in DNS queries and sending them through the DNS protocol, as you can see is our VPS who is anwering to all of them. ","date":"2022-11-13","objectID":"/dns-tunneling/:3:0","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"Android If you want to use all this in an Android device you would need to root it or use Andiodine. Basically with this app we are doing the same as we did in the previous section but without the SSH tunnel part. That means 2 things: The traffic is not encrypted but if you use secure protocols everything should be fine (As usual you know). The routing is not performed by SSH and it is the Linux server the one doing the job. The second point is the problem, we need to make sure that the server is able to forward traffic and also that it is working as a NAT to avoid problems when the packages return. To get all this working I wrote two script that setup IP formarding and NAT before running Iodine. Also they make sure to clean up everything when we close the server. Why two scripts? Well the first one is for hosts without Docker or any container stuff running: #! /bin/bash echo \"Preparing system for ip forwarding...\" echo 1 \u003e /proc/sys/net/ipv4/ip_forward iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE echo iodined -c -f 10.2.0.1 -P PaSSW0RD dns.test.com echo echo \"Cleaning configuration...\" echo 0 \u003e /proc/sys/net/ipv4/ip_forward iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE The second script is for hosts that has Docker running, I know is pretty common so I decided to include this here too since the setup is a bit different: #! /bin/bash echo \"Preparing system for ip forwarding...\" iptables -t nat -A POSTROUTING -s 10.2.0.0/27 -o eth0 -j MASQUERADE iptables -I DOCKER-USER -i dns0 -o eth0 -j ACCEPT iptables -I DOCKER-USER -i eth0 -o dns0 -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT echo iodined -c -f 10.2.0.1 -P PaSSW0RD dns.test.com echo echo \"Cleaning configuration...\" iptables -t nat -D POSTROUTING -s 10.2.0.0/27 -o eth0 -j MASQUERADE iptables -D DOCKER-USER -i dns0 -o eth0 -j ACCEPT iptables -D DOCKER-USER -i eth0 -o dns0 -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT DISCLAIMER: The first script configuration will make the server forward any packet that reach to it, maybe you should consider a more restrictive setup with iptables for production use. The script for Docker hosts only allow the forwarding for packets comming through the DNS tunnel. One more thing, sometimes your VPS provider can disable IP forwarding at the network interface level, make sure to enable it there too following the documentation (Thanks Azure for losing my time figuring this out). Once this is ready, we can setup a conection in the Android application and start using our DNS tunnel without problems in our non-rooted phone! NOTE: This setup could be used through a computer too! You could use the SSH trick we saw before or the script called iodine-client-start that is installed with Iodine to connect to your DNS VPN just as the phone does. ","date":"2022-11-13","objectID":"/dns-tunneling/:3:1","tags":["bypass","red team","network","exfiltration"],"title":"DNS tunneling","uri":"/dns-tunneling/"},{"categories":["Offensive Cybersecurity"],"content":"Post about how Wi-Fi networks can be attacked with the Aircrack-ng suite","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Attacking Wi-Fi networks with Aircrack-ng Before starting, remember that performing any of the attacks explained in this post to networks without previous consent is illegal, this post information is for educational purposes only. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:0:0","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Introduction Wi-Fi uses management frames (datagrams are called frames in this context) and data frames. Only data frames are encrypted and injecting them into the network will require a previous association with the AP. This is not necessary for management frames or if we send data frames spoofing some of the clients MAC addresses. About the hardware you will need, make sure your wireless card support monitor mode. Normally you will need a USB external adapter because internal cards won’t allow you to use them in monitor mode but maybe you are lucky! Another thing to note is that in Kali or Parrot modified drivers that allow us to enable monitor mode in certain cards are already in place but these drivers should be installed manually in other distributions. There are cool resources to know what to buy out there, like lists of adapters or what to look for. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:1:0","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Capturing traffic Before explaining anything more, we need to prepare our network card to be able to capture Wi-Fi packages (monitor mode). It is possible to enable monitor mode with: airmon-ng start \u003ciface\u003e After that, you can now start capturing packages in the air. If the last command worked, you should have a new network interface called mon0, that is what you want to use here as interface: airodump-ng -c \u003cchannel\u003e -w web_attack \u003ciface\u003e Locking a channel will reduce the noise so it is recommended (-bssid option can be used for reducing the noise even more). ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:2:0","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"WEP In this section we will try to learn a bit how WEP works and why it is so vulnerable and then explain how to perform several types of attacks against it. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:3:0","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"How encryption works? WEP uses a keystream to encrypt the traffic: ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:3:1","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Authentication The 802.11 specification describes three possible connection states for a client: Not authenticated Authenticated but not associated Authenticated and associated About how the authentication process work, the 802.11 original standard specified two authentication modes: Open Authentication: If enabled, the client can just send an authentication request frame and the AP (Access point) will send an authentication result with a success state. After that, the association is performed. This mode can be abuse to perform a “fake auth” and being able to associate to an AP without the key. Shared Key Authentication (SKA): The AP send a challenge text (128 bytes) and the client has to encrypt it using a Initialization Vector (IV) and the key and send it back to the AP. The AP verifies that the challenge was correct and authenticate the client. The association process can be done after that. In this case, we can abuse that the challenge packages are sent in plaintext to capture the different packages to calculate a keystream we can use to associate to the AP and also inject packages without knowing the key. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:3:2","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Main flaws Weak authentication scheme Short initialization vector (IV) and subsequent frecuent reuse. Vulnerable to replay attacks Weak frame integrity protection thanks to the linearity of the CRC-32 field used as ICV (Integrity check value) Low resistance to related key attacks enabling efficient statistical attacks. For this, it is necessary a certain amount of captured packages with different IVs. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:3:3","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Attacks Deauthentication Attack The idea is to send a management packet of type deauth with a MAC address to disconnect from the AP. This can be abused to increase network traffic (for key cracking), get the client connected to an evil twinn, capture the challenge process to get a keystream… aireplay-ng -0 10 -c \u003cclient_mac\u003e -a \u003cbssid\u003e \u003ciface\u003e ARP Replay Attacks ARP Replay is the best way of generating more packages in the network to force the creation of new IVs. Once an ARP package is sniffed, it can be re-injected thanks to WEP lack of protection against this type of attacks. Since ARP use broadcast messages, once we are able to re-inject an ARP package the AP will forward it to all the clients. This will generate new IVs for key cracking. It is true that, since the network is encrypted, an attacker is not able to read the traffic but since the ARP packages have a fixed payload size of 36 bytes and always have the broadcast address set in the frame header (FF:FF:FF:FF:FF:FF), it is easy to identify them even encrypted. To perform this attack, we will need to first associtate to the AP. This is how we can perform what is called a “fake auth” abusing open authentication: aireplay-ng -1 15 -a \u003cbssid\u003e -e \u003cssid\u003e \u003ciface\u003e This also can be used instead to avoid getting deauthenticated by picky APs: aireplay-ng -1 6000 -q 10 -o 1 -a \u003cbssid\u003e -e \u003cssid\u003e \u003ciface\u003e Now we can start the attack (Don’t close the previous terminal!): aireplay-ng -3 -b \u003cbssid\u003e \u003ciface\u003e Cracking the key (PWT technique) Everything we have seen is cool but I know we all want one thing: the AP key. For that we will use aircrack-ng. For the tool to work, it needs network packages. 40 bits keys will need about 5000 IVs to be cracked and 104 bits keys could need aroud ten times more. The command needs a password size and as you don’t know the password length at the time of the attack, a good strategy is first trying with 64 bits and if it fails for more that 10000 IVs try with 128 bits. The default key size used is 128 bits (WEP-104). Let’s see how to start a cracking process over the packages we have captured before: aircrack-ng -n \u003ckey_lengh\u003e -e \u003ctarget_ssid\u003e web_attack1*.cap The tool will start reading the captured packages and if the number of IVs is not enough, it will wait until we capture enough of them. Note that this technique only use captured ARP packages to improve the speed of the cracking process so you will need to launch an ARP replay attack in order to get enough IVs for this to work. If everything goes well, you should get the AP key in hexadecimal format. KoreK technique You could want to use the old pre-PWT method to be able to use not only ARP packages when cracking the key. The problem is that you will need much more packages and this method is slower than PWT. In order to use this method just add the -K flag to the aircrack-ng command. Clientless WEP cracking Until now, all the attacks we saw need at least one client connected to the AP in order for them to work. Let’s see how we can get to the cracking step generating ARP packets without clients connected to the network: First authenticate to the AP to get associated to it: aireplay-ng -1 6000 -q 10 -a \u003cbssid\u003e \u003ciface\u003e Now we can launch the aireplay-ng fragmentation attack, the idea is to be able to get a keystream in order to start forging encrypted packets (make sure to use your wireless adapter MAC): aireplay-ng -5 -b \u003cbssid\u003e -c \u003csource_mac\u003e \u003ciface\u003e This will start capturing package until eventually a data frame is sent from the AP, at this point it will start trying to get a valid keystream. Make sure you are associated with the AP and also to be near enough in order for the attack to work. Time to forge an ARP package now, this will create a package and save it into a file: packetforge-ng -0 -a \u003cbssid\u003e -h \u003csource_mac\u003e -k \u003cip1\u003e -l \u003cip2\u003e -y \u003cprga.xor\u003e -w outfile We could use 255.255.255.255 as both ip address and should be fine as most of the AP won’t care. The prga thing is the keystream we g","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:3:4","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"WPA / WPA2 WPA/WPA2 are pretty secure and the attack surface is limited but let’s see what can be done. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:4:0","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Authentication The WPA/WPA2 authentication use a four-way handshake between the client and the AP, this process permits the mutual authentication between the AP (called Authenticator) and the client (called Supplicant). Both parts must know the PSK (pre-shared key) in order for the process to succeeds. During the communication the PSK is never sent through the wireless medium. The PSK is only used to generate a PTK (Pairwise Transient Key) that is used as session-only encryption key. This is how the handshake works: The shared key is used to generate the PMK (Pairwise Master Key). This key is 256 bits long and both the client and the AP independently calculate that key combining the PSK and SSID name. The handshake starts now, the AP sends the client a message containing a nonce. In the WPA specification is called ANonce (Authenticator Nonce). The client now generate another nonce, called SNonce (Supplicant Nonce), and builds the PTK concatenating the PMK, both nonces, the MAC addresses of AP and the client and everything is processed using a cryptographic hash function called PBKDF2-SHA1. Now the client sends its SNonce to the AP so now also the AP can build the PTK. This message also contains a MIC (Message Integrity Code) which is used to authenticate the client. The AP replies to the client with a message containing the GTK (Group Temporal Key) used to decrypt multicast and broadcast traffic. Also a MIC is sent to the client (This message is already encrypted). Lastly, the client sends an ACK to end the authentication process. If you want to learn more about this part, I recommend this article. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:4:1","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Attack NOTE: It is true that in order to connect to a WPA2 network your attack surface is pretty limited but you could use key reinstallation attacks (KRACKs) against vulnerable client devices to decrypt traffic. The only way of attacking WPA is brute force, it is not fancy but is what we have. First we will need to capture a 4-way handshake, in order to that we can start capturing packages as we saw earlier and then deauthenticate a user from the target network. That way the client will try to connect back to the AP and we will get the handshake (Also we could just wait for a new client). There is another method in order to obtain a handshake, airbase-ng. Remember when we use this tool for WEP? Well, thanks to how the WPA handshake works we can start a Rogue AP impersonating a SSID without knowing the password for it (The client send us the MIC in the second step so that is enough) and trick a user to connect to give us the handshake: airbase-ng -c \u003cchannel\u003e -W 1 -Z 4 -e \u003cssid\u003e \u003ciface\u003e The -Z option is used to specify WPA2 options and the 4 stands for CCMP encryption scheme. You will know that a handshake was captured because Airodump-ng will notify you. Now is time for the cracking part, we could use a dictionay attack or a pure brute force attack. aircrack-ng could do the job but it is better if you transform the .cap file to a .hccap file and use Hashcat for better speeds. If you prefer a rainbow table attack you can use Pyrit. There are databases already prepared to use on the internet if you prefer to skip the database building process. Clientless attack If you are lucky, maybe the AP is vulnerable to what is called a PMKID attack. Basically, some APs with roaming features enabled would send to the client something called PMKID to make the re-association process faster. PMKID = HMAC-SHA1-128(PMK, \"PMK Name\" | MAC_AP | MAC_STA) This happens during the first part of the handshake so, an attacker, would be able to capture the PMKID to crack the PSK by just trying to associate to the AP without the interaction of any user. I won´t cover how to do this here because you need some extra stuff but you can find everything you need to know in this hashcat article. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:4:2","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"WPS Wireless Protected Setup (WPS) was designed as a simple and secure method to setup a protected wireless network. WPS provides three different settup alternative methods: Push-Button-Connect Internal-Registrar External-Registrar The first two methods would need pyhisical or web interface access to the AP but the third option only requires the client to know a 8 digits number PIN. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:5:0","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Offensive Cybersecurity"],"content":"Brute Force? Normally, brute forcing an 8 digits number will require testing for $10^8$ (=100.000.000) combinations but the actual form of authentication used by WPS reduces this number. The WPS PIN is divided into two halves of 4 digits each. The last digit of the second half is a checksum meaning it is always calculated from the other digits. The authentication process works like this: Both AP and client initialize keys and internal state. Client provides first half of the PIN. Client provides second half of the PIN. AP sends network configuration, including the network password. The AP will send a NACK packet, terminating the process, if the step 2 or 3 fail. That allow us to perform a pretty efficient brute force attack: We start brute forcing the first half of the PIN incrementing the number to try each time. Once we get to the third step of the authentication process we can repeat the process again with the second half of the PIN. Enjoy your network access! With this approach, we reduced the number of combinations to try to 20.000 (=$10^4$ + $10^4$). If we also take into account that the last digit it is just a checksum value, the number of combinations is even smaller: $10^4$ + $10^3$ (=11.000). There are two tools that can help to exploit this: Reaver Bully We will be using Bully for the brute force attack and a tool called Wash included in Reaver that will allow us to detect vulnerable APs. To start looking for vulnerable APs let’s launch Wash: wash -i mon0 This command also will show if the AP has blocked WPS access due to internal anti-bruteforce protection (This is the major limitation of this attack). In order to start the attack just execute: bully -b \u003cbssid\u003e \u003cinterface\u003e The AP could block WPS after certain failed attempts, to try to avoid this we could add more delay between tries (But this is not perfect). There is an attack called Pixie Dust, it is offline and if the AP is vulnerable you could get the PIN in just minutes thanks to a problem with the random generation of nonces used. Bully offers this attack too with the -d flag so use it if possible to improve your chances. ","date":"2022-09-12","objectID":"/attacking-wi-fi-networks-with-aircrack-ng/:5:1","tags":["wifi","red team","network"],"title":"Attacking Wi-Fi networks with Aircrack-ng","uri":"/attacking-wi-fi-networks-with-aircrack-ng/"},{"categories":["Infrastructure"],"content":"Write up about the process of creating my personal Kubernetes cluster","date":"2022-02-09","objectID":"/magi-project/","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"MAGI project ","date":"2022-02-09","objectID":"/magi-project/:0:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"The idea I have been thinking about the idea of building a cluster made of Raspberrys and update my home infrastructure a bit. Right now, I use a Raspberry Pi 4 B as a little server to run Pi Hole, personal projects, Plex… and also as a NAS. All this sevices are run using Docker containers because I love Docker and the management is easier that way, allowing me to recover from a failure really quick. Since I’m increasing the load to it with more things and some application like Plex sometimes consumes A LOT a cluster could be awesome. After thinking a bit my options I discovered PicoCluster: PicoCluster - Desktop Micro Data Center Kubernetes Docker Cluster Software Advanced Kits and Assembled Cubes come with 4GB Raspberry 4 boards. Starter Kits support 1GB, 2GB and 4GB boards. They have a lot of cool things and one of them is exactly what I needed, a little cluster with 3 Raspberrys. I bought it, if it is not obvious by now, and the plan is to migrate some of the things I have running in my actual Pi to it. The idea is simple, my initial Raspberry will only have essential network services like Pi Hole (Working as DHCP and DNS server), Samba (Used for backups and to add things to Plex) and Netdata for monitoring it 24/7 and alert me if something is going wrong. Meanwhile, the cluster will use that Pi for storage provisioning and will host the rest of application like Plex, Nextcloud, personal projects… Also, as part of this project I want to make sure I can still recover from a terminal failure without a ton of problems and learn a bit more of Ansible. The idea here is that all the cluster initial installation and setup will be performed by an Ansible Playbook that I will create from scratch. ","date":"2022-02-09","objectID":"/magi-project/:1:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"What are you going to read Anthares from the future here! The initial idea sounds great right? A cluster of Raspberrys what a cool thing! Well, looks like migrating all to Kubernetes from a Docker setup is not as straightforward as I thought. Also, while I was working on it I started thinking about improving the security of the new infrastructure and fix some problems I had taking advantage of some Kubernetes features. What I though It was going to be a fast and smooth process (I already have worked with Kubernetes and Helm before) ended up being a really long journey of reading documentation and unexpected problems BUT I really learnt a lot. I tried to cover all the process I followed during my journey so I invite you to relax, get a coffee or something and join me in this adventure, Its going to be a long one. Keep in mind that this is not a Kubernetes tutorial, if you want to follow my steps you are supposed to already know a bit about how things works in Kubernetes. Oh! I almost forgot about it. Keep in mind that the 64 bit version of Raspbian was just released yesterday, 02/04/2022, so obviously during the installation process of this write up I had to install a pre-release version of it. ","date":"2022-02-09","objectID":"/magi-project/:1:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Assembly After waiting a bit (I’m in Spain so it is a long way from the US) a received my order! You can ask them to do the assembly process or even install the applications you want into the cluster to avoid wasting time setting it up but… that is not fun right? The assembly process is documented fairly well in their site and you will only need a screwdriver and some different size heads for it in order to follow all the process. By the way, you can ask them to only send the parts for the cluster but not the actual boards for it. The thing is that with the chip shortage we have right now the price they sell them for is just too good to not get all from them. The assembly took me around 2 hours or so but it was pretty fun to do and it looks incredible: ","date":"2022-02-09","objectID":"/magi-project/:2:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Preparing the SD cards We have a cool looking cube right now, time to bring it to life! The first thing to do is to prepare the SD cards with Raspbian and a basic headless setup (No monitors please). In this case I want to use the 64 bit version of Raspbian (Even though it is still under development and have some issues) because since Apple started using ARM a lot of applications work with ARM64 and can be handy. This Raspbian version can be downloaded from here. I will use the Raspberry Pi imager program to put Raspbian in the SD cards. You can find this software here: Raspberry Pi OS - Raspberry PiFrom industries large and small, to the kitchen table tinkerer, to the classroom coder, we make computing accessible and affordable for everybody. There is an option in the program for custom images that works like a charm: It will take a while just to prepare one card so imagine 3 of them. Once the cards are prepared, we have to do one more thing. Since we don’t want to use a monitor we have to connect the cards to the computer, open the disk called boot and create a file called ssh . This will enable SSH by default. The only problem right now is that the IP address for each Raspberry Pi will be provided by the DHCP server so in the first boot you will need to find them in the network. ","date":"2022-02-09","objectID":"/magi-project/:3:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Initial setup My idea is to create an ansible playbook to make all the initial setup of the cluster, that way if a terminal failure happens I can just get everything setup in the blink of an eye. But as I said above, the Raspberrys will have a random IP in the first boot so I will have to connect to them one by one to setup an static IP address and hostnames. After that, I can add SSH key authentication and from there I can start with the playbook to setup everything. I know it is not perfect because if something goes wrong and I need to reinstall from a clean SD card, I will need to do this whole process of connecting to the Raspberry and make the very first setup by hand before I can use the playbook but I think it is ok for me. Note: I had an error with the 64 bit Raspbian OS about the locale value. I just executed sudo dpkg-reconfigure locales and generated the language that the error was crying about. ","date":"2022-02-09","objectID":"/magi-project/:4:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Cluster setup All the steps described in this section will be included in an Ansible playbook to make this whole process automatic. You can check the playbook here: GitHub - anthares101/k3s-pi-cluster: K3S Pi Cluster project playbookThe monitoring stack used is the Carlos Eduardo version of the kube-prometheus repo and part of the Ansible roles were adapted from Jeff Geerling turing-pi-cluster project. ","date":"2022-02-09","objectID":"/magi-project/:5:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Basic Raspberrys setup Raspbian comes with some stuff configured that we don’t really need, Wi-Fi and Bluetooth for example. To disable them we can just add this to the /boot/config.txt file and reboot: dtoverlay=pi3-disable-wifi dtoverlay=pi3-disable-bt The next thing is to prevent the default pi user from using the sudo command without a password. This is easy, just delete /etc/sudoers.d/010_pi-nopasswd . Also, it would be awesome if the Raspberrys date is correct so set the correct timezone: sudo timedatectl set-timezone \u003cyour_time_zone\u003e For my last trick, I will make some changes to harden the system a bit. The home directory of the pi user is world readable so let’s change that: chmod 0750 /home/pi And since we added a SSH key to the Raspberrys for SSH authentication I will disable the access through SSH using password and also won’t allow root user login. Adding this lines to /etc/ssh/sshd_config and reloading the sshd service will do: PermitRootLogin no UsePAM no PasswordAuthentication no ","date":"2022-02-09","objectID":"/magi-project/:5:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Installing Kubernetes The Kubernetes version I will be installing is K3S: Lightweight KubernetesThe above figure shows the difference between K3s server and K3s agent nodes. For more information, see the architecture documentation. We are a Cloud Native Computing Foundation sandbox project. Should include all the normal Kubernetes features but in a more optimized way. The installation is pretty easy to do actually, it is documented very well in the K3S page and the only thing you have to do is to run an installation script in each node of the cluster. The only thing to keep in mind is that you have to start with the installation of the master node (By default K3S won’t allow you to have more than one) to get a secret token you need for the worker nodes installation. By default the K3S script will deploy some components to the cluster, for a basic testing environment this is ok but if you want to customize this components configuration you need to tell K3S to not deploy them to avoid getting your configuration getting overwrited in every reboot. This page shows what parameters to use in order to customize the installation: K3s Server Configuration ReferenceIn this section, you'll learn how to configure the K3s server. Throughout the K3s documentation, you will see some options that can be passed in as both command flags and environment variables. For help with passing in options, refer to How to Use Flags and Environment Variables. Before starting with the installation I will add this parameters into the /boot/cmdline.txt file of every Pi to make sure the containers work as expected: cgroup_memory=1 cgroup_enable=memory Master node Now we are ready to install K3S in the master node. Since I want to use Prometheus to get the metrics from the nodes instead of the typical Kubernetes metric-server I will use -no-deploy metrics-server to tell K3S to not deploy this component: ./k3s_install.sh --no-deploy metrics-server Once this finish, make sure you get the token for the workers nodes: /var/lib/rancher/k3s/server/node-token and also the kubeconfig file to be able to manage your cluster: /etc/rancher/k3s/k3s.yaml. Worker nodes To add you workers node to the cluster just execute this: K3S_URL=https://\u003cMASTER-NODE-IP\u003e:6443 K3S_TOKEN=\u003cTOKEN\u003e ./k3s_install.sh ","date":"2022-02-09","objectID":"/magi-project/:5:2","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Storage provisioning For persistent volume provisioning I don’t really want to rely in the local path provider that K3S use by default. The problem with that provider is that if a pod is re-scheduled in a different node it won’t be able to access the persistent volume data because that volume is in another node. In order to avoid this problems, I will prepare a NFS server in the original Raspberry Pi I talked you about before and configure a NFS provider in the cluster that points to that server. The NFS server could be in one of the cluster nodes aswell if you don’t have another host to use. NFS server installation To be honest the configuration of the a NFS server was easier than I expected. Just install nfs-kernel-server package and prepare a share folder to use. In the /etc/exports is where you have to configure the share for NFS and also the IPs addreses that can access it. I will let here a little example: /home/pi/Shared MASTER_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) WORKER1_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) WORKER2_IP(rw,fsid=0,all_squash,async,no_subtree_check,anonuid=1000,anongid=1000) Make sure to run sudo exportfs -ra to update NFS exports and also to check that nfs-server and rpc-statd.service are running. ","date":"2022-02-09","objectID":"/magi-project/:5:3","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Cluster configuration Taints and labels So we have a Kubernetes cluster but it needs a bit of tweaking First of all, since I have 3 nodes I want to taint and label the master node to control what applications can be scheduled to it: kubectl taint nodes melchior CriticalAddonsOnly:NoSchedule kubectl label node melchior node-type=master I used that taint because I noticed that all the addons (That is the name K3S use for all the manifests that are put in the /var/lib/rancher/k3s/server/manifests folder for deployment) deployed by K3S use that toleration. The label is one I invented. Tainting the master node will give me control over what pods are scheduled into the master node. The idea is to only allow critical application to be in the master node, this way the master node will be more protected against resource intensive applications that can cause a node to crash. NFS storage provider I will use this NFS provider: GitHub - kubernetes-sigs/nfs-subdir-external-provisionerNFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}. Note: This repository is migrated from https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client. Since it offers the installation through Helm and K3S accepts Helm as a way of deploying an addon, just copying this file to /var/lib/rancher/k3s/server/manifests will be enough: --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: nfs-storage namespace: kube-system spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace: kube-system set: nfs.server: NFS-SERVER-IP nfs.path: NFS-SHARE-PATH storageClass.name: nfs-storage storageClass.accessModes: ReadWriteMany storageClass.reclaimPolicy: Retain storageClass.archiveOnDelete: \"false\" storageClass.defaultClass: \"true\" valuesContent: |- nodeSelector: node-type: master tolerations: - key: CriticalAddonsOnly operator: Exists effect: NoSchedule Notice that I added the needed tolerations and node selector configuration to force Kubernetes to schedule the provider into the master node. After a bit, K3S will deploy all the components of the provider, including a storage class called nfs-storage. This storage class is marked as default but since the pre-installed K3S storage class called local-storage is also marked as default there are 2 options: Delete the local-storage storage class Edit the local-storage storage class to make it non default Traefik custom install The reason why I decided to modify the Traefik configuration is because by default, it won’t be able to get the real IP address of the clients because of the Traefik LoadBalancer configuration. Before you ask, K3S use something called Klipper to create a load balancers inside the cluster. Normally, a load balancer is deployed outside but K3S do it this way to allow the usage of load balancer services easier. Adding externalTrafficPolicy: Local (More information) to the spec section of the Traefik service will solve the problem but if I edit this option in the manifest K3S deploys, our changes won’t persist a reboot. Also I want to add a nodeSelector configuration to make sure Traefik is scheduled in the master node. To achieve this little configuration change, we can create a file called traefik-config.yaml in the same location as the manifests: apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: traefik namespace: kube-system spec: valuesContent: |- service: spec: externalTrafficPolicy: Local nodeSelector: node-type: master And why would I want to do all this may you ask? Why I need the real IP of the clients? Simple, for IP filtering. Using ingresses to access the services in the cluster is great so to expose something to the internet make sense to expose th","date":"2022-02-09","objectID":"/magi-project/:5:4","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Some tips ","date":"2022-02-09","objectID":"/magi-project/:6:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Importing images to K3S Trying to migrate an application I had that uses an image that was not in Dockerhub made me discover that it is possible to just import images to the K3S image store: # Export an image from your Docker image store to a tar file docker save --output test-app-v1.0.0.tar test-app:v1.0.0 # Import an image to the K3S image store in the node sudo k3s ctr images import /home/ubuntu/test-app-v1.0.0.tar The only bad thing is that, each node have its own image store so you have to import the image too all of them in order to avoid problems with scheduling. I guess a much better solution could be to host my own image registry in the cluster o my Raspberry Pi but this works too. ","date":"2022-02-09","objectID":"/magi-project/:6:1","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"K3S snapshots Using the default K3S database, sqlite, makes the creation of snapshots really simple. Just stop the K3S service and copy the entire /var/lib/rancher/k3s/server directory for restoration (Obviously after the copy, start the K3S service again). ","date":"2022-02-09","objectID":"/magi-project/:6:2","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"},{"categories":["Infrastructure"],"content":"Last Thoughts The journey was pretty long and I really had lot of problems to get some things exactly as I wanted. But now that everything is working I can say that the migration was really a success, my old Raspberry Pi can now just focus on essential applications that support my internal network like the Samba, Pihole and NFS servers and the cluster will handle all the rest of the stuff including all those services that I want to expose to the internet. The management of exposed services with Kubernetes is much simplier using network policies and cert-manager and the addition of Cloudflared (I know I could use it in Docker too but let me be happy with it) fixing all the problems with the ports and giving me a WAF with a fancy dashboards is just perfect. PD: Someone noticed the Evangelion references? ","date":"2022-02-09","objectID":"/magi-project/:7:0","tags":["kubernetes","blue team","homelab"],"title":"MAGI Project","uri":"/magi-project/"}]